# ğŸ§  Multimodal Visual Question Answering (VQA) System

A multimodal **Visual Question Answering (VQA)** system that integrates **text, voice, and image** inputs with **multilingual support**, enabling natural humanâ€“AI interaction.  
Developed as part of an **MSc dissertation**, this project demonstrates how **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** can be combined with **multimodal processing** to deliver robust, intelligent question answering.

---

## ğŸš€ Features

- ğŸ—£ï¸ **Multimodal Input:** Accepts text, voice, and image-based queries  
- ğŸŒ **Multilingual Support:** Handles diverse linguistic inputs across multiple languages  
- ğŸ”Š **Audio Processing:** Integrates **Whisper** for accurate voice recognition and noise handling  
- ğŸ’¡ **LLM + RAG Architecture:** Combines reasoning power of LLMs with external knowledge retrieval for context-aware answers  
- ğŸ–¥ï¸ **Interactive Gradio UI:** Clean, accessible web interface for user interaction and testing  

---

## ğŸ§© System Architecture

```text
User Input (Text / Speech / Image)
          â†“
   Audio & Image Processing
          â†“
   Multimodal Fusion Layer
          â†“
   LLM + RAG Inference Engine
          â†“
   Natural Language + Visual Answer
   ```

---

## âš™ï¸ Tech Stack

- **Language:** Python 3.10+
- **Core Framework:** [Gradio](https://gradio.app/) (for the web UI)
- **LLM / Multimodal Models:** OpenAI / VLM-compatible models (e.g. [LLaVA](https://github.com/haotian-liu/LLaVA))
- **Speech-to-Text:** [OpenAI Whisper](https://github.com/openai/whisper) (robust to noise)
- **Retrieval / RAG:**
  - FAISS (vector index) or similar
  - Embeddings model (OpenAI / Sentence Transformers)
- **Image Processing:** [Pillow (PIL)](https://python-pillow.org/), [OpenCV](https://opencv.org/)
- **Environment:** Google Colab / Local GPU
- **Optional:** [`python-dotenv`](https://pypi.org/project/python-dotenv/) for API keys, [`torch`](https://pytorch.org/) for model inference

---

## ğŸ§  Example Use Cases

- **Ask visual questions about uploaded images**  
  _e.g._ â€œWhat is in this picture?â€, â€œWhat colour is the car?â€, â€œHow many people are in the image?â€
- **Use voice input to interact with the model in different languages**  
  _e.g._ speak in Spanish, English, Arabic, or Hindi and get a response in the same or target language
- **Retrieve contextually grounded responses from external documents via RAG**  
  _e.g._ â€œBased on the uploaded report, what are the main findings?â€ or â€œSummarise this image using the product manualâ€

---

## ğŸ“Š Results & Evaluation

The system was evaluated for:

- âœ… **Speech recognition accuracy** under varying noise conditions (microphone distance, background noise, accents)
- âœ… **Multilingual query comprehension** (tested with non-English inputs)
- âœ… **Response relevance and coherence** in multimodal contexts (image + text / speech + image)

---

## ğŸ‘©â€ğŸ’» Role & Contributions

I was responsible for the **end-to-end system design and implementation**, including:

- Developing the **multilingual NLP and audio processing pipeline**
- Integrating **Whisper** for voice input and noise handling
- Building the **Gradio-based user interface** for text / voice / image inputs
- Combining **LLMs with RAG** for enhanced contextual performance
- Conducting **testing, evaluation, and documentation** of the system

- Developing the **multilingual NLP and audio processing pipeline**
- Integrating **Whisper** for voice input and noise handling
- Building the **Gradio-based user interface** for text / voice / image inputs
- Combining **LLMs with RAG** for enhanced contextual performance
- Conducting **testing, evaluation, and documentation** of the system

